{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"type\": \"literal\",\n",
      "        \"answer\": \"Covid-19\",\n",
      "        \"confidence\": 0.5,\n",
      "        \"evidence\": \"Four databases were linked to construct a surveillance-based cohort of adults 18 years of age or older residing in New York State. 10 The Citywide Immunization Registry (CIR) collects and stores all data on Covid-19 vaccine administration for persons residing in New York City, and the New York State Immunization Information System (NYSIIS) collects data for the rest of the state (excluding data that are reported directly to the federal system, such as data for veterans and military personnel and data from the American Indian Health Program). The Electronic Clinical Laboratory Reporting System (ECLRS) collects all reportable Covid-19 test results (nucleic acid amplification testing or antigen testing) in New York State. 19 The Health Electronic Response Data System (HERDS) includes a statewide, daily electronic survey of all inpatient facilities in New York State, which collects data on all new admissions of persons with a laboratory-confirmed Covid-19 diagnosis. The Covid-19 data from the NYSIIS and the CIR were combined and deduplicated on the basis of first name, last name, date of birth, and ZIP Code. The data were then matched to the ECLRS with the use of a deterministic algorithm on the basis of first name, last name, and \",\n",
      "        \"start\": 207,\n",
      "        \"end\": 215\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"literal\",\n",
      "        \"answer\": \"Needle-free\",\n",
      "        \"confidence\": 0.4,\n",
      "        \"evidence\": \"Needle-free, as the name indicates, aims to deliver the dose of a vaccine dose at high velocity into the dermal and subcutaneous layers without penetration by a needle. This method can be considered the newest method for vaccine delivery and holds maximum benefits in terms of waste management as generated needle waste is reduced and the potential risk of accidental needling after injection is eliminated. Needle-free vaccines for multiple and monodoses are currently being explored by researchers across the globe (Lloyd, 2000; Yang et al., 2016) . This swift shift from needle-based to needle-free immunization seems to be ideal for vaccine delivery. During the pandemic/ epidemic situations or global health crises, when mass vaccination is required, needle-free vaccine drug delivery provides ease and facilitates delivery, increasing safety and patient compliance, decreasing cost of medicine, and decreasing pain often associated with injectable vaccines (Giudice and Campbell, 2006; Garg and Aggarwal, 2017) .\",\n",
      "        \"start\": 0,\n",
      "        \"end\": 11\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"literal\",\n",
      "        \"answer\": \"COVID-19\",\n",
      "        \"confidence\": 0.3,\n",
      "        \"evidence\": \"(a) search A (vaccine access search): \\\"state name COVID-19 vaccine coronavirus appointment schedule location provider\\\", with \\\"COVID-19 vaccine\\\" being the exact phrase in the search (b) search B (vaccine reliability search): \\\"state name COVID-19 coronavirus adverse health effects vaccine side illness\\\", with \\\"COVID-19\\\" being the exact phrase in the search.\",\n",
      "        \"start\": 50,\n",
      "        \"end\": 58\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"literal\",\n",
      "        \"answer\": \"antigens\",\n",
      "        \"confidence\": 0.3,\n",
      "        \"evidence\": \"Immunization is one of the most powerful tools for health, but many current vaccines are not affordable, accessible, and acceptable to everyone who needs them. Continual sharpening of this public health tool is needed to achieve the full potential of immunization for improving health. Some advances will come in the form of better vaccine antigens; however, significant potential also lies in improving the way vaccines are packaged and delivered. Reviewing immunization as a package delivery process and recognizing critical hurdles, bottlenecks, and barriers to vaccine flow is a first step toward making immunization programs more efficient and effective. Managing vaccine flow around those obstacles is the day-today work of immunization programs, which often requires heroic effort. This chapter briefly describes key restrictions to vaccine flow logistics in terms of complexity, cost, human resources requirements, distributability, and sources of errors in the immunization process. It reviews new technologies in various stages of development that have the potential to eliminate or reduce restrictions to vaccine flow. These new technologies have the potential to increase the capacity and efficiency of immunization programs and make immunization safer and more effective, affordable, accessible, and acceptable for everyone.\",\n",
      "        \"start\": 340,\n",
      "        \"end\": 348\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"literal\",\n",
      "        \"answer\": \"MVA-S+N\",\n",
      "        \"confidence\": 0.3,\n",
      "        \"evidence\": \"Next, we examined vaccine-induced cellular immune response in mice following I.M. and I.N. immunization. First, vaccine-specific, systemic T-cell response in the spleen was measured using IFN-Î³ T-cell ELISPOT (Fig. 1g) . For the I.M. groups, we observed that MVA vaccination induced significant levels of S-and N-specific T cells in the spleen (mean SFC/10 6 cells for S: 201.5 in vaccine vs. 13.5 in mock; mean SFC/10 6 cells for N: 160 in vaccine vs. 11.5 in mock) (p < 0.01 for S and N) (Fig. 1g) . However, distinct from I.M. immunization, I.N. immunization induced no or little vaccinespecific T-cell response in the spleen (mean SFC for S: 11.2 in vaccine vs. 4.8 in mock; mean SFC for N: 13 in vaccine vs. 5.6 in mock (Fig. 1g) . This was not surprising and consistent with inefficiency of I.N. immunization to induce antibody response in sera (Fig. 1c) . Together, these data support that the MVA-S+N vaccine is immunogenic and induces systemic antibody response (binding IgG) and T-cell response for both S and N proteins in mice following I.M. immunization, although no detectable neutralizing activity was induced. In contrast, I.N. immunization with the vaccine did not appear to induce significant levels of systemic antibody and T-cell responses.\",\n",
      "        \"start\": 901,\n",
      "        \"end\": 908\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def get_evidences(question,max_evidences=3):\n",
    "  params = {}\n",
    "  params['q'] = question\n",
    "  params['max'] = str(max_evidences)\n",
    "  params['wiki'] = \"false\"\n",
    "  params['dbpedia'] = 'false'\n",
    "  params['d4c'] = \"true\"\n",
    "  #r = requests.get('http://drugs4covid.oeg.fi.upm.es/qa/answers', params=params)  \n",
    "  try:\n",
    "    r = requests.get('http://localhost:8000/answers', params=params)  \n",
    "    return r.json()\n",
    "  except e:\n",
    "    print(\"error getting evidences: \" + str(e))\n",
    "    return []  \n",
    "\n",
    "r = get_evidences(\"What is the name of the vaccine that will speed up immunization?\",5)\n",
    "print(json.dumps(r, ensure_ascii=False, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discover possible antiviral treatment for COVID-19\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline('summarization', model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize(text,max_size=150,min_size=50):\n",
    "    text_length = len(text.split(\" \"))\n",
    "    if (text_length > 1024):\n",
    "        return \"empty\"\n",
    "    if (text_length < 10):\n",
    "        return text\n",
    "    if (text_length < max_size):\n",
    "        max_size = text_length - int(text_length/4)\n",
    "    if (max_size < min_size):\n",
    "        min_size = max_size\n",
    "    if (min_size > max_size):\n",
    "        max_size = min_size\n",
    "    summary = summarizer(text, max_length=max_size, min_length=min_size, do_sample=False)\n",
    "    return summary[0]['summary_text'].strip()\n",
    "\n",
    "print(summarize('discover possible antiviral treatment for COVID-19',20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the survey of the survey?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "q_model_name = \"allenai/t5-small-squad2-question-generation\"\n",
    "q_tokenizer = T5Tokenizer.from_pretrained(q_model_name)\n",
    "q_model = T5ForConditionalGeneration.from_pretrained(q_model_name)\n",
    "\n",
    "def get_question(input_string, **generator_args):\n",
    "    input_ids = q_tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = q_model.generate(input_ids, **generator_args)\n",
    "    output = q_tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "print(get_question(\"We Covering exactly 20 years, the survey offers a unique opportunity to understand.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'MISC', 'score': 0.9998902, 'word': 'British', 'start': 0, 'end': 7}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "ner_nlp = pipeline('ner', model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "def get_entities(text):\n",
    "    if (len(text)<1):\n",
    "        return []\n",
    "    result =  ner_nlp(text)\n",
    "    if (len(result)>0):\n",
    "        for r in result:\n",
    "            r['word'] = r['word'].strip()\n",
    "    return result\n",
    "\n",
    "sample_text = \"British researchers\"\n",
    "print(get_entities(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discover possible antiviral treatment for COVID-19']\n"
     ]
    }
   ],
   "source": [
    "def get_contexts(template):\n",
    "    if ('What' not in template):\n",
    "        return []\n",
    "    concepts = template['What']\n",
    "    if ('How' in template):\n",
    "        concepts.extend(template['How'])\n",
    "    contexts = []\n",
    "    for concept in concepts:\n",
    "        sentence = \"\"\n",
    "        #if ('Who' in template):\n",
    "        #    sentence += \" and \".join(template['Who'])\n",
    "        sentence += \" \" + concept + \" \"\n",
    "        if ('Where' in template):\n",
    "            sentence += \"in \" + \" and \".join(template['Where'])\n",
    "        if ('When' in template):\n",
    "            sentence += \" and \".join(template['When'])\n",
    "        contexts.append(sentence.strip())\n",
    "    return contexts\n",
    "\n",
    "input_template = {'Who': ['British'], 'What': ['discover possible antiviral treatment for COVID-19']}\n",
    "print(get_contexts(input_template))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What is Mexico's main cause of death?\", 'What is the main cause of death in Mexico?', 'What is the main cause of death in hospital?']\n"
     ]
    }
   ],
   "source": [
    "def get_questions(contexts):\n",
    "    questions = []\n",
    "    for context in contexts:\n",
    "        summary = summarize (context,max_size=50)\n",
    "        #print(\"Summary:\",summary)\n",
    "        question1 = get_question(summary)[0]\n",
    "        #print(\"Question1:\",question1)\n",
    "        question2 = get_question(context)[0]\n",
    "        #print(\"Question2:\",question2)\n",
    "        if ('?' in question1) and (question1 not in questions):\n",
    "            questions.append(question1)\n",
    "        if ('?' in question2) and (question2 not in questions):\n",
    "            questions.append(question2)\n",
    "    return questions\n",
    "        \n",
    "print(get_questions([\n",
    "    'The main cause of death in Mexico is hospital saturation',\n",
    "    'The main cause of death in is hospital saturation'\n",
    "                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cbadenes/miniforge3/lib/python3.9/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.4.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "#spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_valid(evidence,query_template):\n",
    "    doc = spacy_nlp(evidence)\n",
    "    require_validation = False\n",
    "    ref_entities = []\n",
    "    if (\"Who\" in query_template):        \n",
    "        ref_entities.extend(query_template['Who'])\n",
    "        require_validation = True\n",
    "    if (\"Where\" in query_template):\n",
    "        ref_entities.extend(query_template['Where'])\n",
    "        require_validation = True\n",
    "    if (\"When\" in query_template):\n",
    "        ref_entities.extend(query_template['When'])\n",
    "        require_validation = True\n",
    "    if (not require_validation):\n",
    "        return True\n",
    "    for sent in doc.sents:\n",
    "        for e in get_entities(sent.text):\n",
    "            entity_word = e['word']\n",
    "            if (entity_word in ref_entities):\n",
    "                return True\n",
    "    print(\"INVALID evidence:\",evidence,\"for\",query_template)\n",
    "    return False\n",
    "\n",
    "print(is_valid(\"We Covering exactly 20 years, the survey offers a unique opportunity to understand the long-term dynamics of the Egyptian labour market and its reactions to policy changes (in our case trade policy). The survey is composed of three sections: (i) households; (ii) individuals; (iii) income. The first section, the household questionnaire, is administrated only by the household's head or by the head's spouse. It contains questions on basic demographic characteristics of the members of the household, movements of the household's members as well as questions regarding the ownership of assets and durable goods. The second section, the individual questionnaire, includes questions to which each person answers individually, concerning the educational background, employment and unemployment conditions and its reasons, average wage, job characteristics, mobility, job search activities, migration stories and a complete section on women's work, their condition in the households and fertility. The 2018 wave dedicates more attention to measures of job stability, given the recent trends of the country towards higher precariousness. The third section encompasses all possible sort of income sources, from family-run agricultural and non-agricultural enterprises to transfers and remittances. The survey is representative at the national level. The ELMPS covers of the whole country, dividing it into six different macro-regions: Greater Cairo, Alexandria, Urban Lower Egypt, Urban Upper Egypt, Rural Lower Egypt and Rural Upper Egypt, with the only exception of the Frontier governorates. The final sample included 15,746 households and 61,231 individuals.\",{\"Who\":[\"Egyptian\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6088888\n",
      "0.64697105\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    text = [sentence]\n",
    "    embedding = sentence_model.encode(text)\n",
    "    return embedding[0]\n",
    "\n",
    "def get_cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "v1 = get_embedding('Janssen asks the EU to approve a single-dose vaccine that will speed up immunization')\n",
    "v2 = get_embedding('What is Janssen asking the EU to approve?')\n",
    "v3 = get_embedding('What is the name of the vaccine that will speed up immunization?')\n",
    "\n",
    "print(get_cosine(v1,v2))\n",
    "print(get_cosine(v1,v3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidences_from_annotated_new_item(annotation):\n",
    "    new_text = annotation['text_en']\n",
    "    #ref_embedding = get_embedding(ref_text)\n",
    "    statements = []\n",
    "    for w in a['5w1h_value']:\n",
    "        if (w['5w1h_label'] != 'What'):\n",
    "            statements.append(new_text.replace(w['5w1h_text_en'],\"\"))\n",
    "    print(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvGFQHXjwdnF",
    "outputId": "a8e991f2-1998-46e0-d375-c7eaabfeddde",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import time\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "input_name = \"covid_5w1h_dataset_170_news_english\"\n",
    "input_file = open('../claims/'+input_name+'.json')\n",
    "data = json.load(input_file)\n",
    "total = len(data)\n",
    "counter = 0\n",
    "try: \n",
    "    for i in data:\n",
    "        print(\"[\",counter,\"/\",total,\"]\",i['id'],\":\")\n",
    "        counter += 1        \n",
    "        total_annotations = len(i['annotation_values'])\n",
    "        current_annotation = 0\n",
    "        question_similarity_threshold = 0.6\n",
    "        for a in i['annotation_values']:\n",
    "            get_evidences_from_annotated_new_item(a) \n",
    "        \n",
    "except e:\n",
    "    print(\"Error on execution: \" + str(e))\n",
    "# Closing file\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import time\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "#input_name = \"covid_5w1h_english\"\n",
    "input_file = open('../claims/'+input_name+'.json')\n",
    "output_file = open(\"../evidences/\"+input_name+\"_evidences.json\", \"w\")\n",
    "output_file.write(\"[\\n\")\n",
    "data = json.load(input_file)\n",
    "total = len(data)\n",
    "counter = 0\n",
    "try: \n",
    "    for i in data:\n",
    "        print(\"[\",counter,\"/\",total,\"]\",i['id'],\":\")\n",
    "        counter += 1        \n",
    "        total_annotations = len(i['annotation_values'])\n",
    "        current_annotation = 0\n",
    "        question_similarity_threshold = 0.6\n",
    "        for a in i['annotation_values']:\n",
    "            current_annotation += 1\n",
    "            print(\"\\t annotation\",current_annotation,\"/\",total_annotations,\":\",a['text_en'])\n",
    "            query_template = {}\n",
    "            questions = {}\n",
    "            evidences = {}\n",
    "            ref_text = a['text_en']\n",
    "            ref_embedding = get_embedding(ref_text)\n",
    "            query_sentence = ref_text\n",
    "            for q in get_questions([query_sentence]):\n",
    "                if (q not in questions):\n",
    "                    vector = get_embedding(q)\n",
    "                    similarity = get_cosine(ref_embedding,vector)\n",
    "                    if (similarity > question_similarity_threshold):\n",
    "                        questions[q]=similarity                                                                    \n",
    "            for w in a['5w1h_value']:\n",
    "                question_type =  w['5w1h_label']\n",
    "                question_values = query_template.get(question_type)\n",
    "                if (question_values is None):\n",
    "                    question_values = []\n",
    "                value = w['5w1h_text_en'].replace(\".\",\"\")\n",
    "                if (question_type == \"Who\"):\n",
    "                    for e in get_entities(value):\n",
    "                        if (e['entity_group'] == 'ORG') or (e['entity_group'] == 'PER'):\n",
    "                            question_values.append(e['word'])\n",
    "                elif (question_type == \"Where\"):\n",
    "                    for e in get_entities(value):\n",
    "                        if (e['entity_group'] == 'LOC') :\n",
    "                            question_values.append(e['word'])\n",
    "                else:\n",
    "                    question_values.append(value)\n",
    "                if (len(question_values)>0):\n",
    "                    query_template[question_type]=question_values\n",
    "                query_sentence = a['text_en'].replace(w['5w1h_text_en'],'')\n",
    "                for q in get_questions([query_sentence]):\n",
    "                    if (q not in questions):\n",
    "                        vector = get_embedding(q)\n",
    "                        similarity = get_cosine(ref_embedding,vector) \n",
    "                        questions[q]=similarity\n",
    "            answers = []\n",
    "            sorted_questions = sorted(questions, key=questions.get, reverse=True)\n",
    "            a['questions']=[]\n",
    "            question_threshold = 0.6\n",
    "            for q in sorted_questions:\n",
    "                if (questions[q]>question_threshold):\n",
    "                    a['questions'].append(q)\n",
    "            a['evidences']=[]\n",
    "            for question in a['questions']:\n",
    "                print(\"\\t\\t\\tquestion:\",question,questions[question])\n",
    "                for e in get_evidences(question,2):\n",
    "                    if (is_valid(e['evidence'],query_template)):\n",
    "                        if (e['evidence'] not in a['evidences']):\n",
    "                            a['evidences'].append(e['evidence'])\n",
    "                            print(\"-> [\",e['confidence'],\"]\",e['evidence'])\n",
    "                sleep(randint(1,10))\n",
    "        json_string = json.dumps(i, ensure_ascii=False, indent=4)\n",
    "        output_file.write(json_string)\n",
    "        if (counter < total):\n",
    "            output_file.write(\",\")\n",
    "        output_file.write(\"\\n\")\n",
    "except e:\n",
    "    print(\"Error on execution: \" + str(e))\n",
    "output_file.write(\"\\n]\")\n",
    "\n",
    "# Closing file\n",
    "input_file.close()\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
